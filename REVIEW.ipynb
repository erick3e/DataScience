{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "## Regression measures on model accuracy\n",
    "\n",
    "* MAD (Mean Absolute Deviation)\n",
    "    $$ \\frac{1}{N} \\sum |{\\hat{Y}_i - Y_i }| $$\n",
    "* MAPE Mean Absolute Percentage Error\n",
    "    $$ \\frac{1}{N} \\sum_i |{ \\frac{Y_i -\\hat{Y}_i}{ Y_i } }| $$\n",
    "* SMAPE Symmetric Mean Absolute Percent Error\n",
    "    $$ \\frac{1}{N} \\sum_i { \\frac{2|Y_i -\\hat{Y}_i |}{ |Y_i| + |\\hat{Y}_i| } } $$\n",
    "* Mean Square Error\n",
    "    $$ \\frac{1}{N} \\sum_i {(Y_i - \\hat{Y}_i)^2} $$\n",
    "* Quantile Loss\n",
    "    $$ \\frac{1}{N} \\sum_i {q(Y_i - \\hat{Y}_i)^+ + (1 - q)(\\hat{Y}_i - Y_i) ^+}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Models\n",
    "> Needs a clean dataset no duplicates of features\n",
    "## Regression (Predictions)\n",
    "Your target value $ y $ is `real`. It can be used for forecasting, ratings, KPIs, good vs. bad\n",
    "### Linear regression $ f(x) = b + \\sum {W(x)} $\n",
    "* Simple and good for classification tatsks and neural nets\n",
    "\n",
    "#### Gradient Descent\n",
    "> gradient: the vector of partial derivaties. Gives the input direction in which the fucntion most quickly increases.\n",
    ">> if if a function has multiple minima, it might find the wrong one (try different starting points. if there is no minimum, it might go on forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4b2a65df24b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mderivative_estimate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference_quotient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m#plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    #computes the sum of swaared elements in v\n",
    "    return sum(v_i **2 for v_i in v)\n",
    "\n",
    "def difference_quotient(f,x,h):\n",
    "    # measures how f(x) cahnges when x changfes. Limit of the difference quotients:\n",
    "    return (f(x + h) - f(x))/ h\n",
    "def square(x):\n",
    "    return x * x\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "\n",
    "derivative_estimate = partial(difference_quotient, square, h=0.00001)\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(-10,10)\n",
    "plt.title('Actual derivaties vs. Estimates')\n",
    "plt.plot(x, map(derivative, x), 'rx', label= 'Actual')\n",
    "plt.plot(x, map(derivative_estimate, x), 'b+', label= 'Estimate')\n",
    "plt.legend(loc=9)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Logistic regression\n",
    "* Classification model that splits datasets into 2 buckts and assigns proabbility to belong to each class\n",
    "> Logistic regression computes probability value using sigmoid funtion. Then the values can be mapped to discrete classes.\n",
    "\n",
    "$ Y =0, when x <= 0 and Y = 1 when x > 0$\n",
    "    $$ F(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "#### Cost function for Logistic Regression\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum^N _{i=1} {cost(h_\\theta (x) - y)} $$\n",
    "$$ cost(h_\\theta (x), y) = -log(h_\\theta (x)), y=1 $$\n",
    "$$ cost(h_\\theta (x), y) = -log(1 - h_\\theta (x)), y= 0 $$\n",
    "\n",
    "#### Precision, recall and accuracy\n",
    "* Precision is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
